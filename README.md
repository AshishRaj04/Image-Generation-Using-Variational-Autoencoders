# Variational Autoencoders (VAE)
---
 A Variational Autoencoder (VAE) is a type of generative model that learns to map input data to a latent space and generate new data from this space. It consists of an encoder that compresses the input into latent 
 variables, and a decoder that reconstructs the data from these variables. VAEs introduce a probabilistic element by using the mean and variance of the latent space, allowing for smooth data generation. The model 
 is trained using two losses: reconstruction loss (to measure data accuracy) and KL divergence (to regularize the latent space).
 There is an Autoencoder model and then there is and  Variational Autoencoder model . The major difference being the assuracne of continous latent space in the case of variational autoencoder . It's been explained 
 in the notebook in great detail.
 
![Architecture of VAE]([https://example.com/my_image.png](https://images.app.goo.gl/bK25K4GfrE2ceVj18))

## Accuracy ðŸŽ¯
---
* **Autoencoder** -
* **Variational Autoencoder** -

## Data
---
I have used the commonly available fashion MNIST dataset from [Tensorflow Docs]([https://example.com](https://www.tensorflow.org/datasets/catalog/fashion_mnist))

## Contact ðŸ“©
---

[e-mail](ar4196189@gmail.com)
[linkdin](www.linkedin.com/in/ashish-raj-230239280)


